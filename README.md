# 迁移学习(transfer_learning)
该项目下主要展示迁移学习相关内容：如FastText文本分类、FastText词向量训练、使用FastText进行词向量迁移、迁移学习实践等

## FastText
FastText是由Facebook AI Research (FAIR) 开发的一款高效文本表示和分类工具。它基于Word2Vec的思想，但进行了扩展，能够处理子词信息，适用于多种自然语言处理任务，如文本分类、词向量训练等。
- 字符级别的N-gram: FastText的核心思想是将单词分解成字符级别的n-grams，例如，对于单词 “apple”，当 n=3 时，其3-gram包括：<ap, app, ppl, ple, le>，单词的前后加 < 和 > 是为了区分前缀和后缀。
- 子词信息（Subword Representation）: FastText将每个n-gram视为一个独立的单元，学习它们的向量表示。单词的向量则由组成该单词的所有n-gram向量的平均值得到。
- 基于负采样的skipgram（Skipgram with Negative Sampling（SGNS））: FastText使用SGNS作为训练词向量的方法。SGNS的基本思想是给定一个中心词，预测其上下文词，并采用负采样来加速训练。
- 分层softmax（Softmax Classifier）: 对于文本分类任务，FastText将文本中的所有词向量取平均值，然后输入到一个softmax分类器中进行分类。
### FastText作用
- 进行文本分类
  - 提供了一种高效的文本分类方法，特别适合于大规模数据集。它通过将文本表示为词向量的平均值，结合线性分类器（如 softmax）进行训练和预测。该方法非常快速，能够在短时间内处理大量文本数据，并生成分类模型。
  实现快速而准确的文本分类，包括多分类和二分类。
- 词向量训练
  - 捕捉词汇的语义信息。与Word2Vec不同，FastText还考虑了词的内部结构，因此即使遇到未登录词，也能通过其子词信息生成合理的向量表示。
  - 训练高质量的词向量，可以用于各种下游任务。

### FastText架构
#### 层次softmax
层次softmax（Hierarchical Softmax）是一种用于加速训练的技巧，特别是在处理大规模词汇表时。它通过将 softmax操作转化为一个树形结构（霍夫曼树（Huffman Tree））来减少计算复杂度，从而提升模型的训练效率。

在传统的softmax中，计算目标词的概率需要计算整个词汇表中所有词的概率，这在词汇表非常大的情况下计算开销非常大。层次softmax通过构建一个二叉树，使得每次预测时只需要计算沿路径的几个节点，从而显著降低了计算量。

工作原理:
- 霍夫曼树: 每个叶子节点代表一个词，每个非叶子节点代表一个二元分类问题。 词频高的词在树的较高层，词频低的词在树的较低层。 这使得高频词的路径更短，从而减少计算。
- 路径编码: 从根节点到叶子节点的路径可以被编码为一个二元码 (0 或 1)，其中0代表左子树，1代表右子树。
- 概率计算: 每个非叶子节点代表一个二元分类问题，使用sigmoid函数计算概率。 从根节点到叶子节点的概率是路径上所有节点概率的乘积。

优势:
- 计算效率高：层次Softmax将计算复杂度从O(V)降低到O(logV)，其中V是词汇量的大小。
- 内存效率高：只需要存储树的结构和节点的参数，不需要存储整个词汇表的参数。

#### 负采样
负采样（Negative Sampling）是FastText和其他自然语言处理模型（如Word2Vec）中用于加速训练和提高效率的一种技术。它主要用于解决传统Softmax在大词汇量情况下的计算瓶颈问题。负采样通过简化目标函数，只对少数负样本进行更新，从而大幅减少计算量。

原理:
负采样的核心思想是将多分类问题转化为二分类问题，只更新正样本和少量负样本的参数，而不是整个词汇表。具体来说：
- 正样本: 对于给定的中心词，选取其上下文词作为正样本。
- 负样本: 我们从词汇表中随机选择一些词，作为负样本。 这些词与中心词的上下文词无关。
- 二分类问题: 将每个正样本视为 1，负样本视为 0，训练一个二元分类器来区分中心词和上下文词是否相关。
  - 对于正样本，目标是最大化其概率。
  - 对于负样本，目标是最小化其概率。


## 烹饪文本分类案例
1. 烹饪文本分类案例（数据来源于facebook AI实验室提供的演示数据集）- FastText工具进行文本分类详细过程
   - 数据集处理及划分
   - 模型训练、测试
   - 自动超参数调优
   - 实际生成中多标签多分类问题的损失计算方式
   - 模型保存
## 词向量训练：fasttext工具训练词向量的过程
   - 核心思想
     - FastText的词向量训练基于Skip-gram with Negative Sampling (SGNS)模型。其核心思想是：给定一个中心词，预测其上下文中的词。 通过学习中心词与其上下文词之间的关系，模型能够学习到词的语义表示。与Word2Vec的Skip-gram模型相比，FastText的主要创新在于引入了字符n-gram，将词拆分为字符级别的n-grams，例如 “apple” 的 3-grams为["<ap", "app", "ppl", "ple", "le>"]。这些n-gram和单词本身一起构成单词的最终表示。
   - 优点
     - 处理未登录词 (OOV): 对于未在训练语料中出现的单词，FastText可以通过其n-gram来构建向量表示，从而有效地解决OOV问题。
     - 捕捉形态信息: 对于具有词形变化的语言，字符n-gram可以捕捉到单词内部的形态信息，例如，“running” 和 “run” 之间的关系可以通过共享n-gram来体现。
     - 提高泛化能力: 使用n-gram可以提高模型的泛化能力，使其更好地处理unseen data。
## 词向量迁移：使用fasttext进行词向量模型迁移
   - 概念
     - 词向量迁移也称为词嵌入迁移或预训练词向量的使用，是指将一个预先训练好的词向量模型（通常是在大型语料库上训练的）应用到新的、通常是更小的、特定任务的数据集上的过程。这个过程能够显著提高模型在新任务上的性能，尤其是在数据稀缺的情况下。
   - 优势 
     - 提升性能: 在数据稀缺的情况下，可以显著提高模型的性能。 
     - 加快训练速度: 避免从头训练词向量，可以加快训练速度。 
     - 更好的泛化能力: 预训练模型能够学习到丰富的语义和句法信息，具有更好的泛化能力。
   - FastText工具中可以提供的可迁移的词向量:
     - FastText提供了157种语言的在CommonCrawl和Wikipedia语料上进行训练的可迁移词向量模型, 它们采用CBOW模式进行训练, 词向量维度为300维。可通过该地址查看具体语言词向量模型: https://fasttext.cc/docs/en/crawl-vectors.html
     - FastText提供了294种语言的在Wikipedia语料上进行训练的可迁移词向量模型, 它们采用skipgram模式进行训练, 词向量维度同样是300维. 可通过该地址查看具体语言词向量模型: https://fasttext.cc/docs/en/pretrained-vectors.html
## 迁移学习:迁移学习的两种迁移方式
   - 预训练模型
   - 微调
> 为什么需要豫训练和微调
>- 解决数据稀缺问题: 在数据量较少的情况下，直接训练一个大型模型很容易过拟合。预训练模型学习到的通用特征可以作为良好的初始化，从而提高模型的泛化能力。
>- 加快训练速度: 预训练模型已经学习到了一些有用的特征，因此微调通常比从头开始训练模型更快。
>- 提高模型性能: 预训练模型能够捕捉到数据中的深层模式和规律，微调能够让模型更好地适应新任务，从而提高模型性能。
>- 降低训练成本: 预训练模型通常需要大量的计算资源，但只需训练一次，可以在多个任务中重复使用，从而降低了整体的训练成本。

   |对比项|预训练模型| 微调 |
   |----|----|---|
   |定义|使用已经在大规模数据集上训练好的模型直接进行推理或者迁移。|在预训练模型的基础上对模型进行小范围的调整，适应目标任务。|
   |数据需求|不需要目标任务的数据，可以直接使用预训练模型进行任务执行。|需要目标任务的数据来进行微调，特别是要对模型做一些局部优化。|
   |计算资源|减少训练成本，因为直接使用现成的模型，而不是从头开始训练。|在预训练的基础上做细微调整，减少训练时间，但仍需要一定计算资源。|
   |适用场景|数据极其稀缺的场景，源任务与目标任务相似时（例如，同类型任务）。|当目标任务与源任务有一定差异，但目标任务仍然有一定的数据时。|
   |表现|在目标任务上不一定表现最好，因为没有针对性地进行微调。|微调后，通常能更好地适应目标任务，性能提升显著。|
   |训练方式|直接使用预训练模型进行推理或迁移学习，可能不需要训练。|在预训练模型的基础上进行参数更新，微调模型的部分或全部参数。|

## 迁移学习中transformer库使用
   - 管道（Pipline）方式
   - 自动模型（AutoMode）方式
   - 具体模型（SpecificModel）方式

## 微调的方式进行迁移学习实践
   - 中文文本分类
   - 中文完形填空